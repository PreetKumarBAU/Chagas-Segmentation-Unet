{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dGEIdBplcPxH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-A1vY7NfDT0",
    "outputId": "ed4a94ff-9e48-48e0-9172-371ded6c5ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingPathImagesAre: 600\n",
      "trainingPathImagesAre: 600\n",
      "TestPathImagesAre: 200\n",
      "TestPathImagesAre: 200\n",
      "ValidationPathImagesAre: 200\n",
      "ValidationPathImagesAre: 200\n",
      "Number of images in Total , Validation images and Test Images :  1000 150 150\n",
      "Number of images in Train , Validation images and Test Images :  700 150 150\n",
      "(8, 256, 256, 3) (8, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "def load_data(paths , split = 0.15):\n",
    "  images = []\n",
    "  masks = []\n",
    "  numImages = 0\n",
    "  numMasks = 0\n",
    "\n",
    "  for path in paths:\n",
    "    \n",
    "    images.extend(sorted(glob(os.path.join(path , \"images/*\"))))\n",
    "    masks.extend(sorted(glob(os.path.join(path , \"label/*\"))))\n",
    "    if \"training\" in path:\n",
    "\n",
    "      n = \"training\"\n",
    "    if \"Test\" in path:\n",
    "      n= \"Test\"\n",
    "    if \"Val\" in path:\n",
    "\n",
    "      n= \"Validation\"\n",
    "\n",
    "    print(\"{}PathImagesAre:\".format(n),len(images) - numImages)\n",
    "    print(\"{}PathImagesAre:\".format(n),len(masks) - numMasks)\n",
    "\n",
    "    numImages = len(images)\n",
    "    numMasks = len(masks)\n",
    "\n",
    "  total_size = len(images)\n",
    "  valid_size = int(split * total_size)\n",
    "  test_size = int(split * total_size)\n",
    "  print(\"Number of images in Total , Validation images and Test Images : \" , total_size , valid_size ,test_size  )\n",
    "\n",
    "  train_x , test_x = train_test_split(images , test_size = test_size , random_state = 42 )\n",
    "  train_y , test_y = train_test_split(masks , test_size = test_size , random_state = 42 )\n",
    "\n",
    "  train_x , valid_x = train_test_split(train_x , test_size = valid_size , random_state = 42 )\n",
    "  train_y , valid_y = train_test_split(train_y , test_size = valid_size , random_state = 42 )\n",
    "\n",
    "  return (train_x , train_y) , (valid_x , valid_y ) , (test_x , test_y) \n",
    "\n",
    "def read_image(path):\n",
    "  path = path.decode()\n",
    "  x = cv2.imread(path , cv2.IMREAD_COLOR)\n",
    "  x = cv2.resize(x , (256 , 256))\n",
    "  x = x/255.0\n",
    "  #Size is 256 * 256 * 3\n",
    "\n",
    "  return x\n",
    "\n",
    "def read_mask(path):\n",
    "  path = path.decode()\n",
    "  x = cv2.imread(path , cv2.IMREAD_GRAYSCALE)\n",
    "  x = cv2.resize(x , (256 , 256))\n",
    "  x = x/255.0\n",
    "  \n",
    "  #Size is 256 * 256\n",
    "  x = np.expand_dims(x , axis = -1)\n",
    "  #Size becames 256 * 256 * 1\n",
    "\n",
    "  return x\n",
    "\n",
    "def tf_parse(imagepath , maskpath):\n",
    "  def _parse(imagepath , maskpath):\n",
    "    x = read_image(imagepath)\n",
    "    y = read_mask(maskpath)\n",
    "\n",
    "    return x , y\n",
    "\n",
    "  x , y = tf.numpy_function(_parse , [imagepath , maskpath] , [tf.float64 , tf.float64] )\n",
    "  x.set_shape([256 , 256 , 3])\n",
    "  y.set_shape([256, 256, 1])\n",
    "\n",
    "  return x , y \n",
    "\n",
    "\n",
    "def tf_dataset( imagepath , maskpath , batch = 8):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((imagepath , maskpath))\n",
    "  dataset = dataset.map(tf_parse)\n",
    "  dataset = dataset.batch(batch)\n",
    "  dataset = dataset.repeat()\n",
    "  return dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  paths = [\"/content/drive/MyDrive/training\" , \"/content/drive/MyDrive/Test\", \"/content/drive/MyDrive/Validation\"] \n",
    "  (train_x , train_y) , (valid_x , valid_y ) , (test_x , test_y)  =   load_data(paths)\n",
    "\n",
    "  print(\"Number of images in Train , Validation images and Test Images : \" ,len(train_x) , len(valid_x ) , len(test_x) )\n",
    "\n",
    "  ds = tf_dataset(test_x , test_y)\n",
    "  for x, y in ds:\n",
    "    print(x.shape , y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "soIDHqfyAuxg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "o3N3Cb2wMd8N"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def conv_block(x, num_filters):\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    #x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_model():\n",
    "    size = 256\n",
    "    num_filters = [16, 32, 48, 64 , 128 , 256 , 512]\n",
    "    inputs = Input((size, size, 3))\n",
    "\n",
    "    skip_x = []\n",
    "    x = inputs\n",
    "    ## Encoder\n",
    "    for f in num_filters:\n",
    "        x = conv_block(x, f)\n",
    "        skip_x.append(x)\n",
    "        x = MaxPool2D((2, 2))(x)\n",
    "\n",
    "    ## Bridge\n",
    "    x = conv_block(x, num_filters[-1])\n",
    "\n",
    "    num_filters.reverse()\n",
    "    skip_x.reverse()\n",
    "    ## Decoder\n",
    "    for i, f in enumerate(num_filters):\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        xs = skip_x[i]\n",
    "        x = Concatenate()([x, xs])\n",
    "        x = conv_block(x, f)\n",
    "\n",
    "    ## Output\n",
    "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "\n",
    "    return Model(inputs, x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1rJXtnwKNmyk"
   },
   "outputs": [],
   "source": [
    "# Train \n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
    "from tensorflow.keras.metrics import Recall , Precision \n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    def f(y_true, y_pred):\n",
    "        intersection = (y_true * y_pred).sum()\n",
    "        union = y_true.sum() + y_pred.sum() - intersection\n",
    "        x = (intersection + 1e-15) / (union + 1e-15)\n",
    "        x = x.astype(np.float32)\n",
    "        return x\n",
    "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mj_3zplHGHa",
    "outputId": "7fa6b2bb-8642-4d3b-e211-13462dcf0a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation_models\n",
      "  Downloading https://files.pythonhosted.org/packages/da/b9/4a183518c21689a56b834eaaa45cad242d9ec09a4360b5b10139f23c63f4/segmentation_models-1.0.1-py3-none-any.whl\n",
      "Collecting image-classifiers==1.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/81/98/6f84720e299a4942ab80df5f76ab97b7828b24d1de5e9b2cbbe6073228b7/image_classifiers-1.0.0-py3-none-any.whl\n",
      "Collecting keras-applications<=1.0.8,>=1.0.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
      "\u001b[?25hCollecting efficientnet==1.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.19.5)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0->segmentation_models) (0.16.2)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.5.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.4.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (7.1.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.2.2)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (4.4.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.15.0)\n",
      "Installing collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\n",
      "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n"
     ]
    }
   ],
   "source": [
    "pip install segmentation_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F99FWAusHVJb",
    "outputId": "1535851f-680f-4e69-d9a0-3265ce67b374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6PHOBzJx8E5w"
   },
   "outputs": [],
   "source": [
    "#from segmentation_models import Unet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6XUYN2nNLEK",
    "outputId": "605b5f18-6301-4523-b162-d35f0793e829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "88/88 [==============================] - 353s 4s/step - loss: 1.7617 - acc: 0.5408 - recall_1: 0.5969 - precision_1: 0.0194 - iou: 0.0153 - val_loss: 1.5647 - val_acc: 0.9768 - val_recall_1: 2.8081e-04 - val_precision_1: 5.3748e-04 - val_iou: 0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f66c30635f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
      "    handle=self._handle, deleter=self._deleter)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
      "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "88/88 [==============================] - 17s 196ms/step - loss: 1.4694 - acc: 0.8259 - recall_1: 0.3412 - precision_1: 0.0304 - iou: 0.0166 - val_loss: 1.4389 - val_acc: 0.9816 - val_recall_1: 1.8052e-04 - val_precision_1: 8.7153e-04 - val_iou: 0.0131\n",
      "Epoch 3/30\n",
      "88/88 [==============================] - 20s 226ms/step - loss: 1.4159 - acc: 0.8692 - recall_1: 0.2855 - precision_1: 0.0347 - iou: 0.0172 - val_loss: 1.3646 - val_acc: 0.9839 - val_recall_1: 8.0232e-05 - val_precision_1: 0.0013 - val_iou: 0.0131\n",
      "Epoch 4/30\n",
      "88/88 [==============================] - 17s 195ms/step - loss: 1.3859 - acc: 0.9062 - recall_1: 0.2904 - precision_1: 0.0505 - iou: 0.0182 - val_loss: 1.3261 - val_acc: 0.9848 - val_recall_1: 0.0057 - val_precision_1: 0.7061 - val_iou: 0.0143\n",
      "Epoch 5/30\n",
      "88/88 [==============================] - 17s 194ms/step - loss: 1.3643 - acc: 0.9391 - recall_1: 0.3860 - precision_1: 0.1047 - iou: 0.0199 - val_loss: 1.3037 - val_acc: 0.9848 - val_recall_1: 0.0059 - val_precision_1: 0.8684 - val_iou: 0.0147\n",
      "Epoch 6/30\n",
      "88/88 [==============================] - 17s 195ms/step - loss: 1.3529 - acc: 0.9516 - recall_1: 0.5727 - precision_1: 0.1827 - iou: 0.0231 - val_loss: 1.2982 - val_acc: 0.9848 - val_recall_1: 0.0027 - val_precision_1: 0.9735 - val_iou: 0.0142\n",
      "Epoch 7/30\n",
      "88/88 [==============================] - 17s 193ms/step - loss: 1.3268 - acc: 0.9704 - recall_1: 0.6778 - precision_1: 0.3339 - iou: 0.0262 - val_loss: 1.2889 - val_acc: 0.9853 - val_recall_1: 0.1121 - val_precision_1: 0.8934 - val_iou: 0.0174\n",
      "Epoch 8/30\n",
      "88/88 [==============================] - 20s 222ms/step - loss: 1.3098 - acc: 0.9756 - recall_1: 0.7315 - precision_1: 0.4151 - iou: 0.0294 - val_loss: 1.2751 - val_acc: 0.9845 - val_recall_1: 0.4869 - val_precision_1: 0.6555 - val_iou: 0.0243\n",
      "Epoch 9/30\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2954 - acc: 0.9784 - recall_1: 0.7440 - precision_1: 0.4681 - iou: 0.0319 - val_loss: 1.2671 - val_acc: 0.9854 - val_recall_1: 0.4284 - val_precision_1: 0.7256 - val_iou: 0.0243\n",
      "Epoch 10/30\n",
      "88/88 [==============================] - 17s 193ms/step - loss: 1.2824 - acc: 0.9805 - recall_1: 0.7469 - precision_1: 0.5145 - iou: 0.0339 - val_loss: 1.2521 - val_acc: 0.9862 - val_recall_1: 0.3922 - val_precision_1: 0.8064 - val_iou: 0.0250\n",
      "Epoch 11/30\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2697 - acc: 0.9819 - recall_1: 0.7593 - precision_1: 0.5507 - iou: 0.0362 - val_loss: 1.2441 - val_acc: 0.9863 - val_recall_1: 0.3771 - val_precision_1: 0.8270 - val_iou: 0.0255\n",
      "Epoch 12/30\n",
      "88/88 [==============================] - 19s 221ms/step - loss: 1.2583 - acc: 0.9826 - recall_1: 0.7662 - precision_1: 0.5728 - iou: 0.0380 - val_loss: 1.2279 - val_acc: 0.9864 - val_recall_1: 0.4210 - val_precision_1: 0.8175 - val_iou: 0.0283\n",
      "Epoch 13/30\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2476 - acc: 0.9834 - recall_1: 0.7668 - precision_1: 0.5953 - iou: 0.0397 - val_loss: 1.2181 - val_acc: 0.9864 - val_recall_1: 0.4233 - val_precision_1: 0.8171 - val_iou: 0.0295\n",
      "Epoch 14/30\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2371 - acc: 0.9839 - recall_1: 0.7770 - precision_1: 0.6130 - iou: 0.0417 - val_loss: 1.2135 - val_acc: 0.9856 - val_recall_1: 0.3368 - val_precision_1: 0.7837 - val_iou: 0.0272\n",
      "Epoch 15/30\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2294 - acc: 0.9841 - recall_1: 0.7639 - precision_1: 0.6159 - iou: 0.0426 - val_loss: 1.2050 - val_acc: 0.9862 - val_recall_1: 0.3506 - val_precision_1: 0.8410 - val_iou: 0.0285\n",
      "Epoch 16/30\n",
      "88/88 [==============================] - 20s 221ms/step - loss: 1.2177 - acc: 0.9848 - recall_1: 0.7815 - precision_1: 0.6405 - iou: 0.0449 - val_loss: 1.1938 - val_acc: 0.9863 - val_recall_1: 0.3808 - val_precision_1: 0.8331 - val_iou: 0.0308\n",
      "Epoch 17/30\n",
      "88/88 [==============================] - 17s 192ms/step - loss: 1.2075 - acc: 0.9852 - recall_1: 0.7878 - precision_1: 0.6572 - iou: 0.0467 - val_loss: 1.1843 - val_acc: 0.9864 - val_recall_1: 0.4051 - val_precision_1: 0.8234 - val_iou: 0.0329\n",
      "Epoch 18/30\n",
      "88/88 [==============================] - 17s 193ms/step - loss: 1.1986 - acc: 0.9854 - recall_1: 0.7926 - precision_1: 0.6651 - iou: 0.0485 - val_loss: 1.1814 - val_acc: 0.9862 - val_recall_1: 0.2989 - val_precision_1: 0.8757 - val_iou: 0.0287\n",
      "Epoch 19/30\n",
      "88/88 [==============================] - 20s 222ms/step - loss: 1.1906 - acc: 0.9855 - recall_1: 0.7900 - precision_1: 0.6674 - iou: 0.0499 - val_loss: 1.1796 - val_acc: 0.9858 - val_recall_1: 0.1810 - val_precision_1: 0.9261 - val_iou: 0.0240\n",
      "Epoch 20/30\n",
      "88/88 [==============================] - 20s 223ms/step - loss: 1.1830 - acc: 0.9854 - recall_1: 0.7853 - precision_1: 0.6629 - iou: 0.0510 - val_loss: 1.1743 - val_acc: 0.9857 - val_recall_1: 0.1455 - val_precision_1: 0.9423 - val_iou: 0.0224\n",
      "Epoch 21/30\n",
      "88/88 [==============================] - 17s 193ms/step - loss: 1.1737 - acc: 0.9856 - recall_1: 0.7906 - precision_1: 0.6701 - iou: 0.0526 - val_loss: 1.1621 - val_acc: 0.9862 - val_recall_1: 0.2474 - val_precision_1: 0.9062 - val_iou: 0.0282\n",
      "Epoch 22/30\n",
      "88/88 [==============================] - 20s 222ms/step - loss: 1.1639 - acc: 0.9860 - recall_1: 0.8059 - precision_1: 0.6871 - iou: 0.0552 - val_loss: 1.1487 - val_acc: 0.9865 - val_recall_1: 0.3083 - val_precision_1: 0.8997 - val_iou: 0.0323\n",
      "Epoch 23/30\n",
      "88/88 [==============================] - 20s 221ms/step - loss: 1.1551 - acc: 0.9863 - recall_1: 0.8105 - precision_1: 0.6968 - iou: 0.0571 - val_loss: 1.1476 - val_acc: 0.9862 - val_recall_1: 0.2180 - val_precision_1: 0.9225 - val_iou: 0.0280\n",
      "Epoch 24/30\n",
      "88/88 [==============================] - 20s 223ms/step - loss: 1.1476 - acc: 0.9864 - recall_1: 0.8105 - precision_1: 0.7029 - iou: 0.0588 - val_loss: 1.1412 - val_acc: 0.9863 - val_recall_1: 0.2160 - val_precision_1: 0.9088 - val_iou: 0.0285\n",
      "Epoch 25/30\n",
      "88/88 [==============================] - 17s 193ms/step - loss: 1.1417 - acc: 0.9861 - recall_1: 0.8055 - precision_1: 0.6891 - iou: 0.0601 - val_loss: 1.1183 - val_acc: 0.9867 - val_recall_1: 0.4071 - val_precision_1: 0.8467 - val_iou: 0.0424\n",
      "Epoch 26/30\n",
      "88/88 [==============================] - 20s 222ms/step - loss: 1.1325 - acc: 0.9864 - recall_1: 0.8204 - precision_1: 0.7042 - iou: 0.0628 - val_loss: 1.1208 - val_acc: 0.9864 - val_recall_1: 0.3068 - val_precision_1: 0.8748 - val_iou: 0.0365\n",
      "Epoch 27/30\n",
      "88/88 [==============================] - 17s 194ms/step - loss: 1.1243 - acc: 0.9866 - recall_1: 0.8217 - precision_1: 0.7090 - iou: 0.0647 - val_loss: 1.1210 - val_acc: 0.9861 - val_recall_1: 0.2086 - val_precision_1: 0.9031 - val_iou: 0.0305\n",
      "Epoch 28/30\n",
      "88/88 [==============================] - 20s 223ms/step - loss: 1.1152 - acc: 0.9868 - recall_1: 0.8315 - precision_1: 0.7170 - iou: 0.0673 - val_loss: 1.1093 - val_acc: 0.9863 - val_recall_1: 0.2415 - val_precision_1: 0.9023 - val_iou: 0.0342\n",
      "Epoch 29/30\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.1074 - acc: 0.9868 - recall_1: 0.8337 - precision_1: 0.7169 - iou: 0.0695 - val_loss: 1.1162 - val_acc: 0.9856 - val_recall_1: 0.1089 - val_precision_1: 0.9296 - val_iou: 0.0238\n",
      "Epoch 30/30\n",
      "88/88 [==============================] - 20s 224ms/step - loss: 1.1027 - acc: 0.9864 - recall_1: 0.8240 - precision_1: 0.7006 - iou: 0.0706 - val_loss: 1.0839 - val_acc: 0.9868 - val_recall_1: 0.3422 - val_precision_1: 0.8774 - val_iou: 0.0453\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from segmentation_models.losses import bce_jaccard_loss\n",
    "    from segmentation_models.metrics import iou_score\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    #HYPERPARAMETER\n",
    "\n",
    "\n",
    "    batch = 8\n",
    "    lr = 1e-4\n",
    "    epochs = 2\n",
    "    opt = tf.keras.optimizers.Adam(lr)\n",
    "    metrics = [\"acc\" , Recall() , Precision() , iou]\n",
    "    #Data\n",
    "    train_dataset = tf_dataset(train_x, train_y, batch=batch)\n",
    "    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch)\n",
    "\n",
    "    model = build_model()\n",
    "    #model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss=bce_jaccard_loss, optimizer=opt, metrics=metrics)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\"files/model.h5\"),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "        CSVLogger(\"/content/drive/MyDrive/data.csv\"),\n",
    "        TensorBoard(),\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n",
    "    ]\n",
    "    #Number Of Batches\n",
    "    train_steps = len(train_x)//batch\n",
    "    valid_steps = len(valid_x)//batch\n",
    "\n",
    "    if len(train_x) % batch != 0:\n",
    "        train_steps += 1\n",
    "    if len(valid_x) % batch != 0:\n",
    "        valid_steps += 1\n",
    "\n",
    "    model.fit(train_dataset,\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=valid_steps,\n",
    "        callbacks=callbacks,\n",
    "        shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cjSAGkTaXe5"
   },
   "outputs": [],
   "source": [
    "    model.fit(train_dataset,\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=valid_steps,\n",
    "        callbacks=callbacks,\n",
    "        shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9v9Gzke8b5hI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lF3xdTDvXMCV",
    "outputId": "a7beec5a-a19c-488a-8f03-dc2a5e7dfa57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:13<00:00, 11.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "from tqdm import tqdm\n",
    "#from data import load_data, tf_dataset\n",
    "#from train import iou\n",
    "\n",
    "def read_image(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (256, 256))\n",
    "    x = x/255.0\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (256, 256))\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x\n",
    "\n",
    "def mask_parse(mask):\n",
    "    mask = np.squeeze(mask)\n",
    "    mask = [mask, mask, mask]\n",
    "    mask = np.transpose(mask, (1, 2, 0))\n",
    "    return mask\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## Dataset\n",
    "    \n",
    "    batch_size = 8\n",
    "    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n",
    "\n",
    "    test_dataset = tf_dataset(test_x, test_y, batch=batch_size)\n",
    "\n",
    "    test_steps = (len(test_x)//batch_size)\n",
    "    if len(test_x) % batch_size != 0:\n",
    "        test_steps += 1\n",
    "\n",
    "    #with CustomObjectScope({'iou': iou}):\n",
    "    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n",
    "\n",
    "    #model.evaluate(test_dataset, steps=test_steps)\n",
    "\n",
    "    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n",
    "        h, w, _ = x.shape\n",
    "        white_line = np.ones((h, 10, 3)) * 255.0\n",
    "\n",
    "        all_images = [\n",
    "            x * 255.0, white_line,\n",
    "            mask_parse(y), white_line,\n",
    "            mask_parse(y_pred) * 255.0\n",
    "        ]\n",
    "        image = np.concatenate(all_images, axis=1)\n",
    "\n",
    "        cv2.imwrite(f\"/content/drive/MyDrive/results/{i}.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ImSXxXF-gySO",
    "outputId": "327f07bc-2475-4d46-ffd1-408120d379df"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_85ff5852-9b32-46b0-8d0b-3a02ed7a2874\", \"results\", 4096)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(\"/content/drive/MyDrive/results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "vDBPzdrHeFqJ",
    "outputId": "be55d019-2732-4476-af4e-5629e4cee8fe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfsrZNR7fiBk"
   },
   "outputs": [],
   "source": [
    "os.listdir(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EctArxGJeygL",
    "outputId": "280ec441-687a-4614-93a7-a31ae9c44da7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', 'drive', 'logs', 'files', 'sample_data']"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLdKNRr6e9_8",
    "outputId": "b1f1d697-d7df-46b4-ad60-a80c1c653182"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chagas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
